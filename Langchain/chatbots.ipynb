{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "02de3557",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e48e698",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x0000018AFFBE70D0>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x0000018AFF251550>, model_name='Gemma2-9b-It', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "model = ChatGroq(model=\"Gemma2-9b-It\",groq_api_key = groq_api_key)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ddc42f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I am Gemma, an open-weights AI assistant. I'm a large language model trained by Google DeepMind.\\n\\nHere are some key things to know about me:\\n\\n* **Open-weights:** My weights are publicly accessible, meaning anyone can see and use the underlying code that makes me work. This promotes transparency and collaboration in the AI community.\\n* **Text-only:** I can only communicate through text. I can't generate images, sound, or videos.\\n* **Knowledge cutoff:** My knowledge is based on the data I was trained on, which has a cutoff point. I don't have access to real-time information or the ability to search the internet.\\n* **Created by the Gemma team:** I was developed by a team of engineers and researchers at Google DeepMind.\\n\\nI'm here to help you with a variety of tasks, such as:\\n\\n* **Generating creative content:**\\n\\nI can write stories, poems, and even code.\\n* **Answering your questions:** I can provide information on a wide range of topics, based on my training data.\\n* **Summarizing text:** I can condense large amounts of text into shorter, more manageable summaries.\\n* **Translating languages:** While I primarily communicate in English, I have some ability to translate between other languages.\\n\\nPlease keep in mind that I am still under development and learning. I may not always be perfect, but I'm always working to improve.\\n\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "model.invoke([HumanMessage(content=\"who are you\")]).content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eb2d4cc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"You told me your name is Rudra and that you are an AI Engineer!  ğŸ˜„  \\n\\nIs there anything specific you'd like to talk about related to AI engineering? I'm happy to chat about it.\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 48, 'prompt_tokens': 53, 'total_tokens': 101, 'completion_time': 0.087272727, 'prompt_time': 0.003545578, 'queue_time': 0.248553501, 'total_time': 0.090818305}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run--e523f062-a181-4fe8-9e80-fa9535db42b7-0', usage_metadata={'input_tokens': 53, 'output_tokens': 48, 'total_tokens': 101})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage\n",
    "model.invoke(\n",
    "    [\n",
    "        HumanMessage(content=\"hi , my name is rudra I am AI Engineer\"),\n",
    "        AIMessage(content=\"hello Rudra It's nice to meet you.\"),\n",
    "        HumanMessage(content=\"hey what's my name and what do I do?\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f1c6c9",
   "metadata": {},
   "source": [
    "## Message History\n",
    "we can use a Message History class to wrap our model and make it stateful.This will keep track of inputs and outputs of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "09409a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "store={}\n",
    "def get_session_history(session_id:str)->BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "with_message_history=RunnableWithMessageHistory(model,get_session_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217264e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hello Rudra! ğŸ‘‹ \\n\\nIt's nice to meet you. What can I do for you today? ğŸ˜Š  \\n\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 29, 'prompt_tokens': 15, 'total_tokens': 44, 'completion_time': 0.052727273, 'prompt_time': 0.00194515, 'queue_time': 0.24726218, 'total_time': 0.054672423}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run--6f54d030-33d8-47f7-8e71-9c1727fb2217-0', usage_metadata={'input_tokens': 15, 'output_tokens': 29, 'total_tokens': 44})"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = {\"configurable\": {\"session_id\": \"chat1\"}}\n",
    "with_message_history.invoke(\n",
    "    [HumanMessage(content=\"HI hello I am Rudra\")],\n",
    "    config\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5974b688",
   "metadata": {},
   "outputs": [],
   "source": [
    "config1 ={\"configurable\":{\"session_id\":\"chat1\"}}\n",
    "response=with_message_history.invoke(\n",
    "    [HumanMessage(content=\"what is my name?\")],\n",
    "    config=config1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "aa5a39d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "config1 ={\"configurable\":{\"session_id\":\"chat2\"}}\n",
    "response=with_message_history.invoke(\n",
    "    [HumanMessage(content=\"what is my name?\")],\n",
    "    config=config1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e7257a26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"As an AI, I have no memory of past conversations and do not know your name. If you'd like to tell me your name, I'd be happy to use it! ğŸ˜Š\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 42, 'prompt_tokens': 14, 'total_tokens': 56, 'completion_time': 0.076363636, 'prompt_time': 0.001879069, 'queue_time': 0.24518675999999998, 'total_time': 0.078242705}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run--22eb80ba-08a4-4fd1-9835-00ff7027732e-0', usage_metadata={'input_tokens': 14, 'output_tokens': 42, 'total_tokens': 56})"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab6952d",
   "metadata": {},
   "source": [
    "## Prompt templates\n",
    "Prompt Templates help to turn raw user information into a format that the LLM can work with. In this case, the raw user input is just a message, which we are passing to the LLM. Let's not make that a bit more complicated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "94fbc96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate,MessagesPlaceholder\n",
    "pt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\"You are a helpful assistant.answer all the question to the next of your ability\"),\n",
    "        MessagesPlaceholder(variable_name=\"messages\")\n",
    "    ]\n",
    ")\n",
    "chain=pt|model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "29ec5ee4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hello Rudra, it's nice to meet you!  \\n\\nI'm ready to answer your questions to the best of my ability. Just ask away! ğŸ˜Š  \\n\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 39, 'prompt_tokens': 30, 'total_tokens': 69, 'completion_time': 0.070909091, 'prompt_time': 0.002088659, 'queue_time': 0.24554647100000002, 'total_time': 0.07299775}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run--70739de9-8ad2-4ec4-80bd-e2a0a852aa8b-0', usage_metadata={'input_tokens': 30, 'output_tokens': 39, 'total_tokens': 69})"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"messages\":[HumanMessage(content=\"My name is Rudra\")]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5731a576",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hi Rudra, it's nice to meet you! \\n\\nIt seems like you've introduced yourself a few times. What can I do for you?  \\n\\nI'm ready for any questions you have! ğŸ˜Š  \\n\\n\""
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with_message_history = RunnableWithMessageHistory(chain,get_session_history)\n",
    "\n",
    "config={\"configurable\":{\"session_id\":\"chat3\"}}\n",
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content=\"Hi my name is rudra\")],\n",
    "    config=config\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "adb2d3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\"You are a helpful assistant.answer all the question to the next of your ability in {language}\"),\n",
    "        MessagesPlaceholder(variable_name=\"messages\")\n",
    "    ]\n",
    ")\n",
    "chain = prompt | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "a29d27fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chain.invoke({\"messages\":[HumanMessage(content=\"hi my name is Rudra\")],\"language\":\"Gujrati\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "62092fd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'àª¨àª®àª¸à«àª¤à«‡ àª°à«àª¦à«àª°àªœà«€! \\n\\nàª®àª¨à«‡ àª–à«àª¶à«€ àª›à«‡ àª•à«‡ àª¤àª®à«‡ àª®àª¨à«‡ \"àª•à«‡àªŸàª²à«‡ àª›à«‡\" àª¬à«‹àª²àªµàª¾ àª†àªµà«àª¯àª¾ àª›à«‹. àª¹à«àª‚ àª¤àª®àª¾àª°à«€ àª¸à¤¹à¤¾à¤¯àª• àª¤àª°à«€àª•à«‡ àª¤àª®àª¾àª°àª¾ àª•àª¹à«‡àªµàª¾àª²àª¾ àªªà«àª°àª¤à«àª¯à«‡ àªœàªµàª¾àª¬ àª†àªªàªµàª¾ àª®àª¾àªŸà«‡ àª¤à«ˆàª¯àª¾àª° àª›à«àª‚. \\n\\nàª¤àª®àª¾àª°à«€ àª¸àª®àªœàª¾àªµà«€ àª¶àª•àª¾àª¯ àª¤à«‡àªµàª¾ àªªà«àª°àª¶à«àª¨à«‹ àªªà«‚àª›à«‹, àª¹à«àª‚ àª®àª¾àª°àª¾ àª¶àª•à«àª¯àª¤àª® àªªà«àª°àª¯àª¤à«àª¨àª¥à«€ àª‰àª¤à«àª¤àª° àª†àªªàªµàª¾àª¨à«‹ àªªà«àª°àª¯àª¤à«àª¨ àª•àª°à«€àª¶. ğŸ˜Š\\n\\n'"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "34a39502",
   "metadata": {},
   "outputs": [],
   "source": [
    "with_message_history=RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"messages\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "d4a55404",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'à¤†à¤ªà¤•à¤¾ à¤¨à¤¾à¤® Rudra à¤¹à¥ˆà¥¤  ğŸ˜Š \\n'"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config4 = {\"configurable\": {\"session_id\": \"chat4\"}}\n",
    "response = with_message_history.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"what my name\")], \"language\": \"Hindi\"},\n",
    "    config=config4,\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8d4812",
   "metadata": {},
   "source": [
    "## Managing the Conversation History\n",
    "one important concept to understand when building chatbots is how to manage conversation history. If left unmanaged, the list of messages will grow unbounded and potentially overflow the context window of the LLM.  \n",
    "\n",
    "'trim_messages' helper to reduce how many messages we're sending to the model.trimmer allows us to specify how many tokens we want to keep,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "0a11d8c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\Desktop\\Gen_AI\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content=\"you're a good assistant\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content=\"hi! I'm bob\", additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='hi!', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='haing fun?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='yes!', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import SystemMessage,trim_messages\n",
    "\n",
    "trimmer = trim_messages(\n",
    "    max_tokens=70,\n",
    "    strategy='last',\n",
    "    token_counter=model,\n",
    "    include_system=True,\n",
    "    allow_partial=False,\n",
    "    start_on=\"human\"\n",
    ")\n",
    "\n",
    "messages=[\n",
    "    SystemMessage(content=\"you're a good assistant\"),\n",
    "    HumanMessage(content=\"hi! I'm bob\"),\n",
    "    AIMessage(content='hi!'),\n",
    "    HumanMessage(content=\"haing fun?\"),\n",
    "    AIMessage(content=\"yes!\"),\n",
    "]\n",
    "\n",
    "trimmer.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "10f4661c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'à¤®à¥à¤à¥‡ à¤¨à¤¹à¥€à¤‚ à¤ªà¤¤à¤¾ à¤•à¤¿ à¤¤à¥à¤®à¥à¤¹à¥‡à¤‚ à¤•à¥Œà¤¨ à¤¸à¤¾ à¤†à¤‡à¤¸à¤•à¥à¤°à¥€à¤® à¤ªà¤¸à¤‚à¤¦ à¤¹à¥ˆ!  à¤•à¥à¤¯à¤¾ à¤¤à¥à¤® à¤®à¥à¤à¥‡ à¤¬à¤¤à¤¾ à¤¸à¤•à¤¤à¥‡ à¤¹à¥‹ à¤•à¤¿ à¤¤à¥à¤®à¥à¤¹à¥‡à¤‚ à¤•à¤¿à¤¸ à¤¸à¥à¤µà¤¾à¤¦ à¤®à¥‡à¤‚ à¤†à¤‡à¤¸à¤•à¥à¤°à¥€à¤® à¤ªà¤¸à¤‚à¤¦ à¤¹à¥ˆ? ğŸ¦  \\n\\n'"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "chain=(\n",
    "    RunnablePassthrough.assign(messages=itemgetter(\"messages\")|trimmer) | prompt | model\n",
    ")\n",
    "\n",
    "response = chain.invoke(\n",
    "    {\n",
    "        \"messages\": messages + [HumanMessage(content=\"what ice cream do i like\")],\n",
    "        \"language\": \"Hindi\"\n",
    "    }\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "7d6d02b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## let's wrap this in the Message History\n",
    "with_message_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messages_key = 'messages',\n",
    ")\n",
    "\n",
    "config={\"configurable\":{\"session_id\":\"chat5\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "6f33efd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableWithMessageHistory(bound=RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  messages: RunnableBinding(bound=RunnableLambda(_enter_history), kwargs={}, config={'run_name': 'load_history'}, config_factories=[])\n",
       "}), kwargs={}, config={'run_name': 'insert_history'}, config_factories=[])\n",
       "| RunnableBinding(bound=RunnableLambda(_call_runnable_sync), kwargs={}, config={'run_name': 'check_sync_or_async'}, config_factories=[]), kwargs={}, config={'run_name': 'RunnableWithMessageHistory'}, config_factories=[]), kwargs={}, config={}, config_factories=[], get_session_history=<function get_session_history at 0x0000018AFFC8B600>, input_messages_key='messages', history_factory_config=[ConfigurableFieldSpec(id='session_id', annotation=<class 'str'>, name='Session ID', description='Unique identifier for a session.', default='', is_shared=True, dependencies=None)])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with_message_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce280300",
   "metadata": {},
   "source": [
    "## Vector stores and retrievers\n",
    "\n",
    "These abstractions are designed to support retrieval of data-- from databased and other sources-- for integration with LLM workflows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37ad29c",
   "metadata": {},
   "source": [
    "### Documents\n",
    "Langchain implements a document abstraction,intended to represent a unit of text associated metadata.\n",
    "Page_content: astring representing the content;\n",
    "- metadata: a dict containing arbitraty metadata. The metadata attribute can capture information about the source of the document,\n",
    "individual Document object often represents a chunk of large document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c10d6c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "documents = [\n",
    "    Document(\n",
    "        page_content=\"Dogs are great companions, know for their loylty and freiendliness.\",\n",
    "        metadata={\"source\": \"mammal-pets-doc\"},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Cats are independent pets that often enjoy their own space.\",\n",
    "        metadata={\"source\": \"mammal-pets-doc\"},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Goldfish are popular pets for beginners, requiring relatively simple care.\",\n",
    "        metadata={\"source\": \"fish-pets-doc\"},\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cf7b46f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'mammal-pets-doc'}, page_content='Dogs are great companions, know for their loylty and freiendliness.'),\n",
       " Document(metadata={'source': 'mammal-pets-doc'}, page_content='Cats are independent pets that often enjoy their own space.'),\n",
       " Document(metadata={'source': 'fish-pets-doc'}, page_content='Goldfish are popular pets for beginners, requiring relatively simple care.')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0a365dee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x000001D9722E1250>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x000001D972506B90>, model_name='Llama3-8b-8192', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "load_dotenv()\n",
    "groq_api = os.getenv('GROQ_API_KEY')\n",
    "\n",
    "os.environ['HF_TOKEN'] = os.getenv('HF_TOKEN')\n",
    "LLM = ChatGroq(groq_api_key=groq_api, model=\"Llama3-8b-8192\")\n",
    "LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789e4197",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Admin\\Desktop\\Gen_AI\\.venv\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "embeddings = HuggingFaceEmbeddings(model=\"Zybg/synthetic-clinical-embedding-model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b85498ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "embeddings = HuggingFaceEmbeddings(model=\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a5271483",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_chroma.vectorstores.Chroma at 0x1d972206c10>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_chroma import Chroma\n",
    "\n",
    "vectorstore=Chroma.from_documents(documents,embeddings)\n",
    "vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6027d59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='3ad4b594-528d-4293-af3f-715f064819c6', metadata={'source': 'mammal-pets-doc'}, page_content='Cats are independent pets that often enjoy their own space.'),\n",
       " Document(id='4a0addda-92d4-4339-9967-0788d474d02f', metadata={'source': 'mammal-pets-doc'}, page_content='Cats are independent pets that often enjoy their own space.'),\n",
       " Document(id='b2cc97a8-570b-4898-9a7e-d359519bf4f1', metadata={'source': 'mammal-pets-doc'}, page_content='Dogs are great companions, know for their loylty and freiendliness.'),\n",
       " Document(id='158c6156-f567-4d00-a1a0-cdbf7567d275', metadata={'source': 'mammal-pets-doc'}, page_content='Dogs are great companions, know for their loylty and freiendliness.')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## vectorStores and use Chroma\n",
    "vectorstore.similarity_search(\"cat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0edc16e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='3ad4b594-528d-4293-af3f-715f064819c6', metadata={'source': 'mammal-pets-doc'}, page_content='Cats are independent pets that often enjoy their own space.'),\n",
       " Document(id='4a0addda-92d4-4339-9967-0788d474d02f', metadata={'source': 'mammal-pets-doc'}, page_content='Cats are independent pets that often enjoy their own space.'),\n",
       " Document(id='b2cc97a8-570b-4898-9a7e-d359519bf4f1', metadata={'source': 'mammal-pets-doc'}, page_content='Dogs are great companions, know for their loylty and freiendliness.'),\n",
       " Document(id='158c6156-f567-4d00-a1a0-cdbf7567d275', metadata={'source': 'mammal-pets-doc'}, page_content='Dogs are great companions, know for their loylty and freiendliness.')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await vectorstore.asimilarity_search(\"cat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e16baf",
   "metadata": {},
   "source": [
    "## Retrievers\n",
    "Langchain VectorStore Objects do not subclass Runnable, and so cannot immediately be integrated into Langchain Expression Language chains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0cfa82bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Document(id='f1fcf2b0-0d97-4ca9-9b7e-44682907a6d4', metadata={'source': 'mammal-pets-doc'}, page_content='Cats are independent pets that often enjoy their own space.')],\n",
       " [Document(id='158c6156-f567-4d00-a1a0-cdbf7567d275', metadata={'source': 'mammal-pets-doc'}, page_content='Dogs are great companions, know for their loylty and freiendliness.')]]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import List\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "retriever = RunnableLambda(vectorstore.similarity_search).bind(k=1)\n",
    "retriever.batch([\"cat\",\"dog\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5bb652",
   "metadata": {},
   "source": [
    "Vectorsotes implement an as_retriever method that will generate a Retriever ,specifically a VectorStoreRetriever. These Retrieves include specific search_type and search_kwargs attributes that identify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "538502c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Document(id='b2cc97a8-570b-4898-9a7e-d359519bf4f1', metadata={'source': 'mammal-pets-doc'}, page_content='Dogs are great companions, know for their loylty and freiendliness.')],\n",
       " [Document(id='b4992932-eae6-470f-b81b-5851d3f3f251', metadata={'source': 'fish-pets-doc'}, page_content='Goldfish are popular pets for beginners, requiring relatively simple care.')]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 1}\n",
    ")\n",
    "retriever.batch(['dog','elephant'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bdf5cbc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to the provided context, Goldfish are popular pets for beginners, requiring relatively simple care.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "message=\"\"\"\n",
    "    Answer this question using the provided context only.\n",
    "\n",
    "    {question}\n",
    "\n",
    "    context:\n",
    "    {context}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_messages([(\"human\",message)])\n",
    "\n",
    "rag_chain={\"context\":retriever,\"question\":RunnablePassthrough()} | prompt | LLM\n",
    "\n",
    "response = rag_chain.invoke(\"tell me about Goldfish\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c46f5be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: pydantic 1.10.22\n",
      "Uninstalling pydantic-1.10.22:\n",
      "  Successfully uninstalled pydantic-1.10.22\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\Admin\\Desktop\\Gen_AI\\.venv\\Lib\\site-packages\\~ydantic'.\n",
      "You can safely remove it manually.\n"
     ]
    }
   ],
   "source": [
    "pip uninstall pydantic -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c93379aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pydantic\n",
      "  Using cached pydantic-2.11.7-py3-none-any.whl.metadata (67 kB)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\admin\\desktop\\gen_ai\\.venv\\lib\\site-packages (from pydantic) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\admin\\desktop\\gen_ai\\.venv\\lib\\site-packages (from pydantic) (2.33.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in c:\\users\\admin\\desktop\\gen_ai\\.venv\\lib\\site-packages (from pydantic) (4.14.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\admin\\desktop\\gen_ai\\.venv\\lib\\site-packages (from pydantic) (0.4.1)\n",
      "Using cached pydantic-2.11.7-py3-none-any.whl (444 kB)\n",
      "Installing collected packages: pydantic\n",
      "Successfully installed pydantic-2.11.7\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "google-generativeai 0.8.5 requires google-ai-generativelanguage==0.6.15, but you have google-ai-generativelanguage 0.6.18 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "pip install pydantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1bbabcf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
